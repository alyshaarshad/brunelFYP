{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import csv\n","import numpy as np \n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import binarize"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["filename = \"processed_data.csv\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["dataset = pd.read_csv(filename,sep=\",\",index_col=21)\n","\n","dataset = dataset.dropna(how=\"any\")\n","\n","df = pd.concat([dataset,pd.get_dummies(dataset['slideNumber'], prefix='slideNumber')],axis=1)\n","df.drop(['slideNumber'],axis=1, inplace=True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["UniqueSlides = dataset.slideNumber.unique()\n","DataFrameDict = {elem : pd.DataFrame for elem in UniqueSlides}\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["for key in DataFrameDict.keys():\n","    DataFrameDict[key] = dataset[:][dataset.slideNumber == key]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"df_1 = DataFrameDict[1.0]\ndf_2 = DataFrameDict[2.0]\ndf_3 = DataFrameDict[3.0]\ndf_4 = DataFrameDict[4.0]\ndf_5 = DataFrameDict[5.0]\ndf_6 = DataFrameDict[6.0]\ndf_7 = DataFrameDict[7.0]\ndf_8 = DataFrameDict[8.0]\ndf_9 = DataFrameDict[9.0]\ndf_10 = DataFrameDict[10.0]\ndf_11 = DataFrameDict[11.0]\ndf_12 = DataFrameDict[12.0]"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["slides = [df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_10,df_11,df_12]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"new_datasets= []\nfor i,data in enumerate(slides):\n    for j, info in enumerate(slides):\n        if ((i) == (j)) | ([j, i] in new_datasets):\n            pass\n        else:\n            new_datasets.append([i,j])\n"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["for i,j in new_datasets:\n","    stimuli_1 = str(i+1)\n","    stimuli_2 = str(j+1)\n","    filename = \"processed_paired_stimuli/\"+stimuli_1+\"_\"+stimuli_2+\".csv\"\n","    \n","    dataframe_1 = slides[i]\n","    dataframe_2 = slides[j]\n","    \n","    df = dataframe_1.append(dataframe_2, sort=False)\n","    \n","    df.to_csv(filename, encoding='utf-8', index=False)\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"processed_paired_stimuli//4_6.csv\nprocessed_paired_stimuli//4_7.csv\nprocessed_paired_stimuli//6_7.csv\nprocessed_paired_stimuli//4_5.csv\nprocessed_paired_stimuli//2_3.csv\nprocessed_paired_stimuli//8_9.csv\nprocessed_paired_stimuli//2_6.csv\nprocessed_paired_stimuli//2_10.csv\nprocessed_paired_stimuli//2_11.csv\nprocessed_paired_stimuli//2_7.csv\nprocessed_paired_stimuli//2_5.csv\nprocessed_paired_stimuli//2_12.csv\nprocessed_paired_stimuli//2_4.csv\nprocessed_paired_stimuli//6_11.csv\nprocessed_paired_stimuli//6_10.csv\nprocessed_paired_stimuli//1_2.csv\nprocessed_paired_stimuli//5_6.csv\nprocessed_paired_stimuli//6_12.csv\nprocessed_paired_stimuli//5_7.csv\nprocessed_paired_stimuli//1_3.csv\nprocessed_paired_stimuli//1_7.csv\nprocessed_paired_stimuli//4_12.csv\nprocessed_paired_stimuli//3_5.csv\nprocessed_paired_stimuli//8_12.csv\nprocessed_paired_stimuli//3_4.csv\nprocessed_paired_stimuli//1_6.csv\nprocessed_paired_stimuli//1_4.csv\nprocessed_paired_stimuli//4_11.csv\nprocessed_paired_stimuli//3_6.csv\nprocessed_paired_stimuli//8_11.csv\nprocessed_paired_stimuli//8_10.csv\nprocessed_paired_stimuli//11_12.csv\nprocessed_paired_stimuli//3_7.csv\nprocessed_paired_stimuli//4_10.csv\nprocessed_paired_stimuli//1_5.csv\nprocessed_paired_stimuli//1_8.csv\nprocessed_paired_stimuli//1_11.csv\nprocessed_paired_stimuli//1_10.csv\nprocessed_paired_stimuli//1_9.csv\nprocessed_paired_stimuli//3_9.csv\nprocessed_paired_stimuli//1_12.csv\nprocessed_paired_stimuli//3_8.csv\nprocessed_paired_stimuli//3_12.csv\nprocessed_paired_stimuli//7_8.csv\nprocessed_paired_stimuli//7_9.csv\nprocessed_paired_stimuli//3_11.csv\nprocessed_paired_stimuli//5_9.csv\nprocessed_paired_stimuli//5_8.csv\nprocessed_paired_stimuli//3_10.csv\nprocessed_paired_stimuli//2_9.csv\nprocessed_paired_stimuli//2_8.csv\nprocessed_paired_stimuli//7_12.csv\nprocessed_paired_stimuli//7_10.csv\nprocessed_paired_stimuli//7_11.csv\nprocessed_paired_stimuli//4_9.csv\nprocessed_paired_stimuli//9_10.csv\nprocessed_paired_stimuli//5_10.csv\nprocessed_paired_stimuli//10_12.csv\nprocessed_paired_stimuli//5_11.csv\nprocessed_paired_stimuli//9_11.csv\nprocessed_paired_stimuli//4_8.csv\nprocessed_paired_stimuli//6_8.csv\nprocessed_paired_stimuli//10_11.csv\nprocessed_paired_stimuli//5_12.csv\nprocessed_paired_stimuli//9_12.csv\nprocessed_paired_stimuli//6_9.csv\n"}],"source":["directory = \"processed_paired_stimuli/\"\n","for subdir, dirs, files in os.walk(directory):\n","    for file in files:\n","        if file.endswith(\".csv\"):\n","            \n","            filepath = subdir + os.sep + file\n","            print(filepath)\n","        \n","            file = pd.read_csv(filepath,sep=\",\",index_col=0)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"slide_1 , slide_2 = file['slideNumber'].unique()\n\n"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"slideNumber\nUnnamed: 0             \n16063               6.0\n16064               6.0\n16065               6.0\n16066               6.0\n16067               6.0\n...                 ...\n6194                9.0\n6195                9.0\n6196                9.0\n6197                9.0\n6198                9.0\n\n[2573 rows x 1 columns]\n"}],"source":"X = file.iloc[:, :-2]\n\nY = file.iloc[:, -1:]\n\nY = pd.DataFrame(binarize(Y, threshold=slide_1, copy=True))\n\nx_train, x_test,y_train ,y_test = train_test_split(X,Y,test_size=0.2, random_state=42)"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>2568</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>2569</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>2571</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>2572</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2573 rows Ã— 1 columns</p>\n</div>","text/plain":"        0\n0     0.0\n1     0.0\n2     0.0\n3     0.0\n4     0.0\n...   ...\n2568  1.0\n2569  1.0\n2570  1.0\n2571  1.0\n2572  1.0\n\n[2573 rows x 1 columns]"},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":"Y"},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":"from keras.models import Sequential\nfrom keras.layers import Flatten\nfrom keras.layers import Dense,Dropout\n\ndef createModel():\n    model = Sequential()\n    \n    model.add(Dense(20, activation=\"relu\", input_shape = (20,)))        \n   \n              \n    model.add(Dense(5, activation= \"relu\"))\n\n    \n    model.add(Dense(1, activation= \"sigmoid\"))\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    \n    return model\n"},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.86434108 0.87596899 0.86434108 0.80620158 0.83720928 0.89883268\n"," 0.81322956 0.85992217 0.92578125 0.765625  ]\n","0.8511452674865723\n"]}],"source":["from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","neural_network = KerasClassifier(build_fn= createModel, \n","                                 epochs=10, \n","                                 batch_size=100, \n","                                 verbose=0)\n","kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n","results = cross_val_score(neural_network, X, Y, cv=kfold)\n","\n","print(results)\n","print(results.mean())\n"]},{"cell_type":"code","execution_count":370,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_26\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_78 (Dense)             (None, 21)                462       \n","_________________________________________________________________\n","dense_79 (Dense)             (None, 5)                 110       \n","_________________________________________________________________\n","dense_80 (Dense)             (None, 1)                 6         \n","=================================================================\n","Total params: 578\n","Trainable params: 578\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 1646 samples, validate on 412 samples\n","Epoch 1/20\n","1646/1646 [==============================] - 2s 1ms/step - loss: 0.5984 - accuracy: 0.6458 - val_loss: 0.3880 - val_accuracy: 0.6723\n","Epoch 2/20\n","1646/1646 [==============================] - 0s 247us/step - loss: 0.3588 - accuracy: 0.7296 - val_loss: 0.3606 - val_accuracy: 0.7306\n","Epoch 3/20\n","1646/1646 [==============================] - 0s 237us/step - loss: 0.3111 - accuracy: 0.8226 - val_loss: 0.3400 - val_accuracy: 0.7524\n","Epoch 4/20\n","1646/1646 [==============================] - 0s 220us/step - loss: 0.2823 - accuracy: 0.8670 - val_loss: 0.2636 - val_accuracy: 0.8689\n","Epoch 5/20\n","1646/1646 [==============================] - 0s 200us/step - loss: 0.2361 - accuracy: 0.9247 - val_loss: 0.3057 - val_accuracy: 0.8058\n","Epoch 6/20\n","1646/1646 [==============================] - 0s 216us/step - loss: 0.2166 - accuracy: 0.9386 - val_loss: 0.1962 - val_accuracy: 0.9854\n","Epoch 7/20\n","1646/1646 [==============================] - 0s 219us/step - loss: 0.1825 - accuracy: 0.9654 - val_loss: 0.1685 - val_accuracy: 0.9660\n","Epoch 8/20\n","1646/1646 [==============================] - 0s 231us/step - loss: 0.1585 - accuracy: 0.9629 - val_loss: 0.1407 - val_accuracy: 0.9733\n","Epoch 9/20\n","1646/1646 [==============================] - 1s 428us/step - loss: 0.1750 - accuracy: 0.9484 - val_loss: 0.1223 - val_accuracy: 0.9879\n","Epoch 10/20\n","1646/1646 [==============================] - 1s 337us/step - loss: 0.1252 - accuracy: 0.9745 - val_loss: 0.1542 - val_accuracy: 0.9369\n","Epoch 11/20\n","1646/1646 [==============================] - 0s 256us/step - loss: 0.1120 - accuracy: 0.9733 - val_loss: 0.0970 - val_accuracy: 0.9854\n","Epoch 12/20\n","1646/1646 [==============================] - 0s 274us/step - loss: 0.1033 - accuracy: 0.9763 - val_loss: 0.1418 - val_accuracy: 0.9417\n","Epoch 13/20\n","1646/1646 [==============================] - 0s 265us/step - loss: 0.0832 - accuracy: 0.9860 - val_loss: 0.0862 - val_accuracy: 0.9733\n","Epoch 14/20\n","1646/1646 [==============================] - 0s 270us/step - loss: 0.1019 - accuracy: 0.9666 - val_loss: 0.0850 - val_accuracy: 0.9733\n","Epoch 15/20\n","1646/1646 [==============================] - 0s 251us/step - loss: 0.0812 - accuracy: 0.9824 - val_loss: 0.1513 - val_accuracy: 0.9272\n","Epoch 16/20\n","1646/1646 [==============================] - 1s 343us/step - loss: 0.1219 - accuracy: 0.9605 - val_loss: 0.0913 - val_accuracy: 0.9879\n","Epoch 17/20\n","1646/1646 [==============================] - 1s 413us/step - loss: 0.0664 - accuracy: 0.9848 - val_loss: 0.0477 - val_accuracy: 1.0000\n","Epoch 18/20\n","1646/1646 [==============================] - 1s 332us/step - loss: 0.0805 - accuracy: 0.9781 - val_loss: 0.0633 - val_accuracy: 0.9854\n","Epoch 19/20\n","1646/1646 [==============================] - 0s 263us/step - loss: 0.0592 - accuracy: 0.9842 - val_loss: 0.0902 - val_accuracy: 0.9660\n","Epoch 20/20\n","1646/1646 [==============================] - 1s 331us/step - loss: 0.0713 - accuracy: 0.9836 - val_loss: 0.0394 - val_accuracy: 1.0000\n","515/515 [==============================] - 0s 47us/step\n"]}],"source":["model1 = createModel()\n","batch_size = 10\n","epochs = 20\n","model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model1.summary()\n","\n","history = model1.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n","\n","\n","loss , accuracy = model1.evaluate(X_test, y_test)\n","\n","\n","#predictions = model1.predict(X_test)\n","\n","\n","#which_slide = np.asarray(list(Y))\n","\n","\n","\n"]},{"cell_type":"code","execution_count":238,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import confusion_matrixfrom sklearn import metrics"]},{"cell_type":"code","execution_count":262,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":340,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import ComplementNB\n","\n","clf = ComplementNB() # calls naive bayes algorithm\n","clf.fit(x_train, np.ravel(y_train)) #train the algorithm\n","ComplementNB()\n","k = clf.predict(x_test) #use the trained model to predict using the test data"]},{"cell_type":"code","execution_count":342,"metadata":{},"outputs":[{"data":{"text/plain":["array([[240,  75],\n","       [ 26, 174]])"]},"execution_count":342,"metadata":{},"output_type":"execute_result"}],"source":["conf_mat = confusion_matrix(y_test, k) # output confusion matrix of true and false positives\n","conf_mat"]},{"cell_type":"code","execution_count":343,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.74031008 0.54263566 0.51162791 0.65116279 0.61627907 0.91439689\n"," 0.85603113 0.37743191 0.609375   0.69921875]\n"]}],"source":["scores = cross_val_score(clf, X, np.ravel(Y), cv=10, scoring='accuracy') # cross validation 10 fold and outputting the results for each fold\n","print(scores)"]},{"cell_type":"code","execution_count":344,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1039,  536],\n","       [ 360,  638]])"]},"execution_count":344,"metadata":{},"output_type":"execute_result"}],"source":["y_pred = cross_val_predict(clf, X, np.ravel(Y), cv=10) #training model using cross validation \n","conf_mat = confusion_matrix(Y, y_pred) # output the matrix \n","conf_mat"]},{"cell_type":"code","execution_count":377,"metadata":{},"outputs":[],"source":["import statistics"]},{"cell_type":"code","execution_count":372,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import KFold, cross_val_score\n","\n","k_fold = KFold(n_splits=10)\n","\n","nb_accuracy = cross_val_score(clf, X, np.ravel(Y), cv=k_fold, n_jobs=1)"]},{"cell_type":"code","execution_count":382,"metadata":{},"outputs":[],"source":["y_pred = cross_val_predict(clf, X, np.ravel(Y), cv=10) #training model using cross validation \n","conf_mat_crossval = confusion_matrix(Y, y_pred) "]},{"cell_type":"code","execution_count":383,"metadata":{},"outputs":[],"source":["av_mean = statistics.mean(nb_accuracy)"]},{"cell_type":"code","execution_count":397,"metadata":{},"outputs":[],"source":["Dict = { } \n","Dict[\"fileName\"] = filename, {\"Neural_Network\": \n","                      {\"loss\":loss,\n","                       \"accuracy\":accuracy , \n","                       \"confusion_matrix\":[]\n","                      } , \n","                  \"Naive_Bayes\": \n","                      {\"scores\": nb_accuracy ,\n","                       \"average_accuracy\": av_mean ,\n","                       \"confusion_matrix\": conf_mat_crossval } \n","                 }"]},{"cell_type":"code","execution_count":406,"metadata":{},"outputs":[{"data":{"text/plain":["0.43410852713178294"]},"execution_count":406,"metadata":{},"output_type":"execute_result"}],"source":["Dict[\"fileName\"][1][\"Naive_Bayes\"][\"scores\"][2]"]},{"cell_type":"code","execution_count":362,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[" "]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}